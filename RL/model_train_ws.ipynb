{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tentacle training with js frontend through websocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### imports ######\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from PPO import PPO\n",
    "from tentacle import TentacleEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### initialize environment hyperparameters ######\n",
    "env_name = \"Custom Tentacle-v1\"\n",
    "\n",
    "has_continuous_action_space = True      # continuous action space; else discrete\n",
    "\n",
    "max_ep_len = 1000                       # max timesteps in one episode\n",
    "max_training_timesteps = int(3e6)       # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "## Note : print/log frequencies should be > than max_ep_len\n",
    "print_freq = max_ep_len * 10            # print avg reward in the interval (in num timesteps)\n",
    "log_freq = max_ep_len * 2               # log avg reward in the interval (in num timesteps)\n",
    "save_model_freq = int(1e5)              # save model frequency (in num timesteps)\n",
    "\n",
    "action_std = 0.6                        # starting std for action distribution (Multivariate Normal)\n",
    "action_std_decay_rate = (0.05)          # linearly decay action_std (action_std = action_std - action_std_decay_rate)\n",
    "min_action_std = 0.1                    # minimum action_std (stop decay after action_std <= min_action_std)\n",
    "\n",
    "action_std_decay_freq = int(2.5e5)      # action_std decay frequency (in num timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### PPO hyperparameters #######\n",
    "update_timestep = max_ep_len * 4        # update policy every n timesteps\n",
    "K_epochs = 80                           # update policy for K epochs in one PPO update\n",
    "\n",
    "eps_clip = 0.2                          # clip parameter for PPO\n",
    "gamma = 0.99                            # discount factor\n",
    "\n",
    "lr_actor = 0.0003                       # learning rate for actor network\n",
    "lr_critic = 0.001                       # learning rate for critic network\n",
    "\n",
    "random_seed = 0                         # set random seed if required (0 = no random seed)\n",
    "\n",
    "# state space dimension\n",
    "state_dim = 14\n",
    "\n",
    "# action space dimension\n",
    "action_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### log files for multiple runs are NOT overwritten\n",
    "log_dir = \"PPO_logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "log_dir = log_dir + \"/\" + env_name + \"/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "#### get number of log files in log directory\n",
    "run_num = 0\n",
    "current_num_files = next(os.walk(log_dir))[2]\n",
    "run_num = len(current_num_files)\n",
    "\n",
    "#### create new log file for each run\n",
    "log_f_name = log_dir + \"/PPO_\" + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
    "\n",
    "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
    "print(\"logging at : \" + log_f_name)\n",
    "#####################################################\n",
    "\n",
    "################### checkpointing ###################\n",
    "run_num_pretrained = (\n",
    "    0  #### change this to prevent overwriting weights in same env_name folder\n",
    ")\n",
    "\n",
    "directory = \"PPO_preTrained\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "directory = directory + \"/\" + env_name + \"/\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(\n",
    "    env_name, random_seed, run_num_pretrained\n",
    ")\n",
    "print(\"save checkpoint path : \" + checkpoint_path)\n",
    "#####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"============================================================================================\")\n",
    "\n",
    "print(\"training environment name : \" + env_name)\n",
    "\n",
    "############# print all hyperparameters #############\n",
    "print(\n",
    "    \"--------------------------------------------------------------------------------------------\"\n",
    ")\n",
    "print(\"max training timesteps : \", max_training_timesteps)\n",
    "print(\"max timesteps per episode : \", max_ep_len)\n",
    "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
    "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
    "print(\n",
    "    \"printing average reward over episodes in last : \"\n",
    "    + str(print_freq)\n",
    "    + \" timesteps\"\n",
    ")\n",
    "print(\n",
    "    \"--------------------------------------------------------------------------------------------\"\n",
    ")\n",
    "print(\"state space dimension : \", state_dim)\n",
    "print(\"action space dimension : \", action_dim)\n",
    "print(\n",
    "    \"--------------------------------------------------------------------------------------------\"\n",
    ")\n",
    "if has_continuous_action_space:\n",
    "    print(\"Initializing a continuous action space policy\")\n",
    "    print(\n",
    "        \"--------------------------------------------------------------------------------------------\"\n",
    "    )\n",
    "    print(\"starting std of action distribution : \", action_std)\n",
    "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
    "    print(\"minimum std of action distribution : \", min_action_std)\n",
    "    print(\n",
    "        \"decay frequency of std of action distribution : \"\n",
    "        + str(action_std_decay_freq)\n",
    "        + \" timesteps\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Initializing a discrete action space policy\")\n",
    "print(\n",
    "    \"--------------------------------------------------------------------------------------------\"\n",
    ")\n",
    "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\")\n",
    "print(\"PPO K epochs : \", K_epochs)\n",
    "print(\"PPO epsilon clip : \", eps_clip)\n",
    "print(\"discount factor (gamma) : \", gamma)\n",
    "print(\n",
    "    \"--------------------------------------------------------------------------------------------\"\n",
    ")\n",
    "print(\"optimizer learning rate actor : \", lr_actor)\n",
    "print(\"optimizer learning rate critic : \", lr_critic)\n",
    "if random_seed:\n",
    "    print(\n",
    "        \"--------------------------------------------------------------------------------------------\"\n",
    "    )\n",
    "    print(\"setting random seed to \", random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "#####################################################\n",
    "print(\n",
    "    \"============================================================================================\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WebSocket is running in a separate thread\n"
     ]
    }
   ],
   "source": [
    "### from copilot\n",
    "import asyncio\n",
    "import websockets\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Create a thread-safe queue to hold messages to be sent\n",
    "message_queue = queue.Queue()\n",
    "ws_uri = \"ws://localhost:8888\"\n",
    "\n",
    "\n",
    "async def send_messages(websocket):\n",
    "    while True:\n",
    "        message = await asyncio.get_event_loop().run_in_executor(\n",
    "            None, message_queue.get\n",
    "        )\n",
    "        if message is None:\n",
    "            break\n",
    "        await websocket.send(json.dumps(message))\n",
    "\n",
    "\n",
    "async def run_ws(ws_uri):\n",
    "    async with websockets.connect(ws_uri) as websocket:\n",
    "        # Start the send_messages coroutine\n",
    "        send_task = asyncio.create_task(send_messages(websocket))\n",
    "\n",
    "        # Example of receiving messages\n",
    "        async for message in websocket:\n",
    "            print(f\"Received message: {message}\")\n",
    "\n",
    "        # Wait for the send_messages coroutine to finish\n",
    "        await send_task\n",
    "\n",
    "\n",
    "def start_event_loop(loop):\n",
    "    asyncio.set_event_loop(loop)\n",
    "    loop.run_forever()\n",
    "\n",
    "\n",
    "# Start the asyncio event loop in a separate thread\n",
    "loop = asyncio.new_event_loop()\n",
    "ws_thread = threading.Thread(target=start_event_loop, args=(loop,))\n",
    "ws_thread.start()\n",
    "\n",
    "# Schedule the run_ws coroutine to run in the event loop\n",
    "asyncio.run_coroutine_threadsafe(run_ws(ws_uri), loop)\n",
    "\n",
    "# Your main code can continue running here\n",
    "print(\"WebSocket is running in a separate thread\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## websocket send message test\n",
    "## use message_queue.put() to send messages\n",
    "message_queue.put(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize a PPO agent\n",
    "ppo_agent = PPO(\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    lr_actor,\n",
    "    lr_critic,\n",
    "    gamma,\n",
    "    K_epochs,\n",
    "    eps_clip,\n",
    "    has_continuous_action_space,\n",
    "    action_std,\n",
    ")\n",
    "\n",
    "## initialize environment\n",
    "env = TentacleEnv(2, 32, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Training #############\n",
    "\n",
    "# track total training time\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "\n",
    "print(\n",
    "    \"============================================================================================\"\n",
    ")\n",
    "# logging file\n",
    "log_f = open(log_f_name, \"w+\")\n",
    "log_f.write(\"episode,timestep,reward\\n\")\n",
    "\n",
    "# printing and logging variables\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "log_running_reward = 0\n",
    "log_running_episodes = 0\n",
    "\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "############# Training Loop #############\n",
    "while time_step <= max_training_timesteps:\n",
    "\n",
    "    state = env.reset()\n",
    "    current_ep_reward = 0\n",
    "\n",
    "    for t in range(1, max_ep_len + 1):\n",
    "\n",
    "        # select action with policy\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done = env.step(action)\n",
    "\n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "        time_step += 1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        # update PPO agent\n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent.update()\n",
    "\n",
    "        # if continuous action space; then decay action std of ouput action distribution\n",
    "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
    "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "        # log in logging file\n",
    "        if time_step % log_freq == 0:\n",
    "\n",
    "            # log average reward till last episode\n",
    "            log_avg_reward = log_running_reward / log_running_episodes\n",
    "            log_avg_reward = round(log_avg_reward, 4)\n",
    "\n",
    "            log_f.write(\"{},{},{}\\n\".format(i_episode, time_step, log_avg_reward))\n",
    "            log_f.flush()\n",
    "\n",
    "            log_running_reward = 0\n",
    "            log_running_episodes = 0\n",
    "\n",
    "        # printing average reward\n",
    "        if time_step % print_freq == 0:\n",
    "\n",
    "            # print average reward till last episode\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "            print(\n",
    "                \"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(\n",
    "                    i_episode, time_step, print_avg_reward\n",
    "                )\n",
    "            )\n",
    "\n",
    "            print_running_reward = 0\n",
    "            print_running_episodes = 0\n",
    "\n",
    "        # save model weights\n",
    "        if time_step % save_model_freq == 0:\n",
    "            print(\n",
    "                \"--------------------------------------------------------------------------------------------\"\n",
    "            )\n",
    "            print(\"saving model at : \" + checkpoint_path)\n",
    "            ppo_agent.save(checkpoint_path)\n",
    "            print(\"model saved\")\n",
    "            print(\n",
    "                \"Elapsed Time  : \",\n",
    "                datetime.now().replace(microsecond=0) - start_time,\n",
    "            )\n",
    "            print(\n",
    "                \"--------------------------------------------------------------------------------------------\"\n",
    "            )\n",
    "\n",
    "        # break; if the episode is over\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "\n",
    "    log_running_reward += current_ep_reward\n",
    "    log_running_episodes += 1\n",
    "\n",
    "    i_episode += 1\n",
    "############# End of Training Loop #############\n",
    "log_f.close()\n",
    "# env.close()\n",
    "\n",
    "# print total training time\n",
    "print(\n",
    "    \"============================================================================================\"\n",
    ")\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "print(\"Started training at (GMT) : \", start_time)\n",
    "print(\"Finished training at (GMT) : \", end_time)\n",
    "print(\"Total training time  : \", end_time - start_time)\n",
    "print(\n",
    "    \"============================================================================================\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basicML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
